{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75029318-8969-4499-84b4-47fe3342a218",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### RDD (Resilient Distributed Dataset)\n",
    "##### Terminologies\n",
    "* RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.\n",
    "\n",
    "##### RDDs are...\n",
    "\n",
    "* immutable\n",
    "* fault tolerant / automatic recovery\n",
    "* can apply multiple ops on RDDs\n",
    "##### RDD operation are...\n",
    "\n",
    "* Transformation\n",
    "* Action\n",
    "##### Basic Operations (Ops)\n",
    "* count(): Number of elements in the RDD is returned.\n",
    "* collect(): All the elements in the RDD are returned.\n",
    "* foreach(f): input callable, and returns only those elements which meet the condition of the function inside foreach.\n",
    "* filter(f): input callable, and returns new RDDs containing the elements which satisfy the given callable\n",
    "* map(f, preservesPartitioning = False): A new RDD is returned by applying a function to each element in the RDD\n",
    "* reduce(f): After performing the specified commutative and associative binary operation, the element in the RDD is returned.\n",
    "* join(other, numPartitions = None): It returns RDD with a pair of elements with the matching keys and all the values for that particular key.\n",
    "* cache(): Persist this RDD with the default storage level (MEMORY_ONLY). You can also check if the RDD is cached or not\n",
    "\n",
    "\n",
    "##### Narrow transformations\n",
    "* map()\n",
    "* filter()\n",
    "* flatMap(j\n",
    "* distinct()\n",
    "##### Wide (Broad) transformations\n",
    "* reduce()\n",
    "* groupby()\n",
    "* sortBy()\n",
    "* join()\n",
    "##### Actions\n",
    "* count()\n",
    "* take()\n",
    "* takeOrdered()\n",
    "* top()\n",
    "* collect()\n",
    "* saveAsTextFile()\n",
    "* first()\n",
    "* reduce()\n",
    "* fold()\n",
    "* aggregate()\n",
    "* foreach()\n",
    "\n",
    "\n",
    "##### Dictionary functions\n",
    "* keys()\n",
    "* values()\n",
    "* keyBy()\n",
    "##### Functional transformations\n",
    "* mapValues()\n",
    "* flatMapValues()\n",
    "##### Grouping, sorting and aggregation\n",
    "* groupByKey()\n",
    "* reduceByKey()\n",
    "* foldByKey()\n",
    "* sortByKey()\n",
    "##### Joins\n",
    "* join()\n",
    "* leftOuterJoin()\n",
    "* rightOuterJoin()\n",
    "* fullOuterJoin()\n",
    "* cogroup()\n",
    "* cartesian()\n",
    "##### Set operations\n",
    "* union()\n",
    "* intersection()\n",
    "* subtract()\n",
    "* subtractByKey()\n",
    "\n",
    "##### Numeric RDD\n",
    "* min()\n",
    "* max()\n",
    "* sum()\n",
    "* mean()\n",
    "* stdev()\n",
    "* variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6b9b51-3d86-4ab1-b0a5-90159e645c96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Lambda Function\n",
    "* A lambda function is a small anonymous function.\n",
    "* A lambda function can take any number of arguments, but can only have one expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad5bd76-ecb6-4ca2-a06b-f4b5109a55d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def my_sum(a,b):\n",
    "  \n",
    "  return a+b\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3cbf35-6e9a-4bd7-88b3-04e9901bfa27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: 30"
     ]
    }
   ],
   "source": [
    "my_sum(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "050e97d5-2cd0-456d-81f6-f2b74fe33377",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 1 µs, total: 13 µs\nWall time: 17.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a=55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b81c16c-ce5c-4124-98e6-f7d2577a1434",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2681ce-0351-41be-9d1b-f6187e857ff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 8"
     ]
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c8c334-eda3-4df1-8a39-adf0919058e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def my_func(a,b):\n",
    "  return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d712124-d211-4719-b56e-bcc217bacdc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 132"
     ]
    }
   ],
   "source": [
    "a=55\n",
    "b=77\n",
    "my_func(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b161228-75e1-4cca-9713-e49c8e8a1b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n30\n"
     ]
    }
   ],
   "source": [
    "x = lambda a : a + 10\n",
    "print(x(3))\n",
    "print(x(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2f559c-4a42-4b02-9ad3-869b783e91bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n52\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = lambda a, b : a + b\n",
    "print(x(5, 6))\n",
    "print(x(2, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db62d86-0d0a-4b1e-8d67-6504a5cba323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n2\n3\n"
     ]
    }
   ],
   "source": [
    "list_a=[1,2,3,4,5,6,7,8]\n",
    "for i in list_a:\n",
    "\tif i<4:\n",
    "\t\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c68b212-b12d-423b-9ee3-0c7a0ede14a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [1, 2, 3, 4, 5, 6, 7, 8, 8, 34, 3, 34, 34, 343, 5656]"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,8,34,3,34,34,343,5656]\n",
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac488506-41b5-4a25-90ad-4cff6db85673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [1, 2, 3, 4, 5, 6, 7, 8, 9, 23, 23, 232323, 2323]"
     ]
    }
   ],
   "source": [
    "my_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,23,23,232323,2323])\n",
    "my_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2417044f-7d74-42e1-ae2f-ee3ebf4d492a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7,8],4)\n",
    "new_rdd = x.filter(lambda x: x<4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1f2043-eeb0-4a67-9986-083e6eb33f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [[1, 2], [3, 4], [5, 6], [7, 8]]"
     ]
    }
   ],
   "source": [
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1b548d-c54b-4638-8590-961dedf3f9bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 3"
     ]
    }
   ],
   "source": [
    "new_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f747306-c857-4e71-92c8-3c9ab4d1a0c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: pyspark.rdd.PipelinedRDD"
     ]
    }
   ],
   "source": [
    "type(new_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff1ec40-fea2-4d8c-80fc-bc279578685d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: pyspark.rdd.PipelinedRDD"
     ]
    }
   ],
   "source": [
    "type(new_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab08a5a-3c2c-42f5-b024-53052bdba517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: dbfs:/databricks-datasets/SPARK_README.md MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0"
     ]
    }
   ],
   "source": [
    "text_rdd = sc.textFile(\"dbfs:/databricks-datasets/SPARK_README.md\")\n",
    "text_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f9b4bd-1fb4-4349-a5c6-3daaded76db6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: pyspark.rdd.RDD"
     ]
    }
   ],
   "source": [
    "type(text_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9fad137-231d-4b69-97c4-7807fc8c887c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3737989515766111>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-3737989515766111>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mc2VsZWN0ICogZnJvbSBzYW1wbGVfZGIuZW1wIHdoZXJlIGRlcHRubz0xMA==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sample_db`.`emp` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n",
       "'Project [*]\n",
       "+- 'Filter ('deptno = 10)\n",
       "   +- 'UnresolvedRelation [sample_db, emp], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3737989515766111>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-3737989515766111>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mc2VsZWN0ICogZnJvbSBzYW1wbGVfZGIuZW1wIHdoZXJlIGRlcHRubz0xMA==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sample_db`.`emp` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'Filter ('deptno = 10)\n   +- 'UnresolvedRelation [sample_db, emp], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sample_db`.`emp` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'Filter ('deptno = 10)\n   +- 'UnresolvedRelation [sample_db, emp], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from sample_db.emp where deptno=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bae8f18a-0862-4d77-bb09-73ab91e27eb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is sparkContext\n",
    "* A SparkContext represents the connection to a Spark cluster, \n",
    "* and can be used to create RDDs, accumulators and broadcast variables on that cluster\n",
    "* Note: Only one SparkContext should be active per JVM. You must stop() the active SparkContext before creating a new one. \n",
    "* param: config a Spark Config object describing the application configuration. \n",
    "* Any settings in this config overrides the default configs as well as system properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a0ea97-61bc-455e-b837-e4dc88cacb90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating RDD using SparkContext Parallelize Method.\n",
    "* Creating list [1,2,3,4,5,6,7,8,9,10]\n",
    "* The sc.parallelize() method is the SparkContext's parallelize method to create a parallelized collection\n",
    "* This allows Spark to distribute the data across multiple nodes, instead of depending on a single node to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2b4118-d28c-451e-a04f-21fcb82f8733",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa51eef-be8d-4493-815a-24f586cf5356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
     ]
    }
   ],
   "source": [
    "range_rdd = sc.range(10)\n",
    "range_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af2851c9-bee9-400e-bb47-b03941d52dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating RDD using file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9696b94-613e-4fd0-9323-10ad9e381d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: 65"
     ]
    }
   ],
   "source": [
    "textFile_rdd = sc.textFile(\"/databricks-datasets/samples/docs/README.md\")\n",
    "textFile_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43118daf-6dd9-4b71-a5b9-7419464c1623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating RDD using SparkContext \n",
    "* Applying MAP Transformation in RDD.\n",
    "* MAP(func)\n",
    "* Return a new distributed dataset formed by passing each element of the source through a function func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea30fef9-3b1d-4cd2-8b0b-87a8cbdd87e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n[11, 12, 13, 14, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "x=sc.parallelize([1,2,3,4,5,6,7])\n",
    "y=x.map(lambda a: a+10 )\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1c247d-743b-466e-b4ae-57e18713046e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X RDD VAlues : [1, 2, 3, 4, 5, 6, 7]\nY RDD Values after applying map and lambda transformation [11, 12, 13, 14, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "print(\"X RDD VAlues :\",x.collect())\n",
    "print(\"Y RDD Values after applying map and lambda transformation\",y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b748d2f-6e29-40a3-871c-4a88d7711455",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n[1, 3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "x_rdd=sc.parallelize([1,2,3,4,5,6,7])\n",
    "y_odd_rdd=x_rdd.filter(lambda z: z%2==1 )\n",
    "print(x_rdd.collect())\n",
    "print(y_odd_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc721901-e0d5-45df-8961-9220c6040e2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: [1, 3, 5, 7]"
     ]
    }
   ],
   "source": [
    "y_odd_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b08f1d-782e-4a15-ad1d-8b73072bd686",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD X values :  [1, 2, 3, 4, 5, 6, 7]\nRDD Y VALUES :  [11, 12, 13, 14, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "print('RDD X values : ',x.collect())\n",
    "print('RDD Y VALUES : ',y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a4029c-5425-4ed3-9a07-4202129e8fe3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### filter(func) Transformation\n",
    "* Return a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fc91df-e72a-4c87-88b9-de32e9fdf255",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD X Values Before Filter :  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nRDD Y Values After apllying Filter in X RDD :  [1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "y = x.filter(lambda x: x%2 == 1) #keep odd values\n",
    "print('RDD X Values Before Filter : ',x.collect())\n",
    "print('RDD Y Values After apllying Filter in X RDD : ',y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fc478a3-c089-4ecd-8908-6c73abb097c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### flatMap(func)  Transformation\n",
    "* Similar to map, but each input item can be mapped to 0 or more output items \n",
    "* (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec643d45-7e38-473d-872f-0ee9c716182e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: [1,\n 56,\n 400,\n 2,\n 57,\n 800,\n 3,\n 58,\n 1200,\n 4,\n 59,\n 1600,\n 5,\n 60,\n 2000,\n 6,\n 61,\n 2400,\n 7,\n 62,\n 2800,\n 8,\n 63,\n 3200,\n 9,\n 64,\n 3600,\n 10,\n 65,\n 4000]"
     ]
    }
   ],
   "source": [
    "x.flatMap(lambda x: (x,x+55,x*400)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2fd4da5-0a8d-474d-b7ee-0a3c216a36cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n[(1, 100), (2, 200), (3, 300)]\n[1, 100, 2, 200, 3, 300]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "y_map = x.map(lambda x: (x, x*100))\n",
    "z_flatmap = x.flatMap(lambda x: (x, x*100))\n",
    "print(x.collect())\n",
    "print(y_map.collect())\n",
    "print(z_flatmap.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d7cc6d9-91e2-4672-b095-ae0170a5b83b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: [1, 2, 3, 4, 5, 6, 7, 8, 9]"
     ]
    }
   ],
   "source": [
    "#When using PySpark, the flatMap() function does the flattening for us.\n",
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "sc.parallelize(data).flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd8213d3-223e-4a23-bbad-1a3bb66839e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### groupBy(func) Transformation in RDD\n",
    "* Group the data in the original RDD. Create pairs where the key is the output of\n",
    "* a user function, and the value is all items for which the function yields this key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ff7bfd-01e3-45b1-8c22-cf2416fda113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Group By ['John', 'Fred', 'Anna', 'James', 'Jan', 'Frend', 'Axe']\nAfter group by  [('J', ('John', 'James', 'Jan')), ('F', ('Fred', 'Frend')), ('A', ('Anna', 'Axe'))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(['John', 'Fred', 'Anna', 'James','Jan','Frend','Axe'])\n",
    "y = x.groupBy(lambda w: w[0])\n",
    "print('Before Group By',x.collect())\n",
    "#print(y.collect())\n",
    "print('After group by ',[(k,tuple(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02793458-228b-4459-a8f1-0192ecdacf0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ravi', 'Sridhar', 'Prasad', 'Raj']\n[('R', ['Ravi', 'Raj']), ('S', ['Sridhar']), ('P', ['Prasad'])]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(['Ravi', 'Sridhar', 'Prasad', 'Raj'])\n",
    "y = x.groupBy(lambda w: w[0])\n",
    "print(x.collect())\n",
    "print([(k, list(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e714e7e-499d-4c0c-9a8a-152990fd0b97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is iterable\n",
    "* anything that can be looped over (i.e. you can loop over a string or file) or\n",
    "* anything that can appear on the right-side of a for-loop: for x in iterable: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a51a3a-6fc6-4c7c-8575-8fd4d551156e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('R', <pyspark.resultiterable.ResultIterable object at 0x7f138e5abbb0>), ('S', <pyspark.resultiterable.ResultIterable object at 0x7f138e5bb100>), ('P', <pyspark.resultiterable.ResultIterable object at 0x7f138e5bb1f0>)]\n"
     ]
    }
   ],
   "source": [
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff6590b-2cef-4739-8caf-ce1604113e92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Sample Transformation\n",
    "* Return a sampled subset of this RDD.\n",
    "\n",
    "* __Parameters__\n",
    "* withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "\n",
    "* fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0,    with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "\n",
    "* seed – seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a21fb7d-8655-46e7-ac31-3db87c152933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: [1, 2, 3]"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7])\n",
    "x.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2475ec3-f65d-4a78-bc02-49b5cc7a75a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10]\n67\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "y = x.sample(True,5,300)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(y.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e241d24-e0e6-4659-b6b1-2bed5eda71d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Union(DataSet) Transformation\n",
    "* Return a new dataset that contains the union of the elements in the source dataset and the argument.\n",
    "* glom() Return an RDD created by coalescing all elements within each partition into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ce9fc4-2e2f-4343-aa6e-b219e51a7be4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#UNION ALL ( it will give all records including duplicates and its same as UNION IN PYSPARK)\n",
    "#UNION (it will eliminate duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6842ffaf-9283-46c4-b207-86b6d4634c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = sc.parallelize([3,4,5,6,7])\n",
    "z = y.subtract(x)\n",
    "print(z.collect())\n",
    "#print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa730ef-e428-4665-8a5a-ed880ca3ab98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### intersection(otherDataset)\n",
    "* Return a new RDD that contains the intersection of elements in the source dataset and the argument.\n",
    "* The output will not contain any duplicate elements, even if the input RDDs did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eb0d42-09c0-472d-afad-46b833e2af0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [3, 4, 5]]\n[[3, 4, 4, 5, 6, 7]]\n[3, 4, 5]\n[[3], [4], [5]]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,3,4,5], 2)\n",
    "y = sc.parallelize([3,4,4,5,6,7], 1)\n",
    "z = x.intersection(y)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n",
    "print(z.collect())\n",
    "print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10636685-0b60-40d2-b033-4298fc63bd61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n[[3], [], []]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3], 2)\n",
    "y = sc.parallelize([3,4], 1)\n",
    "z = x.intersection(y)\n",
    "print(z.collect())\n",
    "print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21827607-7053-4d60-9b87-37b748569d1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### substract(otherdataset) Transformation\n",
    "* It returns an RDD that has only value present in the first RDD and not in second RDD.\n",
    "* its returns if first RDD is having any duplicates. its wont remove any duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4166e4e-1833-4f76-b869-c2a160d8bf53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 2, 2, 8]\n[[], [1, 7], [2, 2, 8]]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,2,3,3,4,5,7,8], 2)\n",
    "y = sc.parallelize([3,4,3,4,5,6], 1)\n",
    "z = x.subtract(y)\n",
    "print(z.collect())\n",
    "print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "863608a1-010a-42a2-82e2-98652ccaaa35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cartesian Transformation\n",
    "* Provides cartesian product of 2 RDDs\n",
    "* like it will return new RDD multiplication 1st RDD each value Into 2nd RDD each Value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cffdb4a-1c39-4a76-8888-5e7913bf135f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9], 2)\n",
    "y = sc.parallelize([3,4,5,6,7,8,9,10,11], 1)\n",
    "z = x.cartesian(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e664ac9-8c46-44d9-8c9e-02bb1286aedf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Distinct Transformation\n",
    "* Return a new RDD containing distinct items from the original RDD (omitting all duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b610fac2-a105-4e1b-a4c9-4ffd2d3dbf21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1, 9, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,3,4,5,5,5,6,6,6,6,7,7,7,3,2,1,8,9,4])\n",
    "y = x.distinct()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f39c681-abc2-44c0-ac12-250c4e4f1c56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### coalesce(numPartitions)\t Transformation\n",
    "* Decrease the number of partitions in the RDD to numPartitions. \n",
    "* Useful for running operations more efficiently after filtering down a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca31667-b36f-4b77-b1e8-852646fc26e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4], [5, 6], [7, 8, 9, 5], [4, 5], [6, 8], [9, 10, 12, 14], [12, 12], [14, 15], [16, 44, 44, 45]]\n[[1, 2, 3, 4], [5, 6, 7, 8, 9, 5, 4, 5], [6, 8, 9, 10, 12, 14], [12, 12, 14, 15, 16, 44, 44, 45]]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 4, 5, 6, 8, 9, 10, 12, 14, 12, 12, 14, 15, 16, 44, 44, 45]\nCPU times: user 20.4 ms, sys: 7.87 ms, total: 28.3 ms\nWall time: 622 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = sc.parallelize([1, 2, 3, 4, 5,6,7,8,9,5,4,5,6,8,9,10,12,14,12,12,14,15,16,44,44,45], 10)\n",
    "y = x.coalesce(4)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n",
    "print(x.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b42a39e1-379b-4070-ba63-e7dce71fcb8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /user/hive/warehouse/mydb.db/dept_csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ddaee05-6242-4b84-aa71-b70a31db8f29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### repartition(numPartitions)\tTransformation\n",
    "* Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. \n",
    "* This always shuffles all data over the network. Repartition by column We can also repartition by columns.\n",
    "* syntax: `repartition(numPartitions, *cols)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d825c8d-eef6-4a32-9b3d-086e06999d93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [12, 434, 545, 65, 3434], [232, 4545, 234234, 33, 434, 3434]]\n5\n[[11, 12, 13, 14, 15], [], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [], [], [232, 4545, 234234, 33, 434, 3434], [12, 434, 545, 65, 3434], []]\n8\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,12,434,545,65,3434,232,4545,234234,33,434,3434], 5)\n",
    "y = x.repartition(8)\n",
    "print(x.glom().collect())\n",
    "print(x.getNumPartitions())\n",
    "print(y.glom().collect())\n",
    "print(y.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccfe34d9-5558-4d55-9e44-29f0f3f831a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PartitionBy Transformation\n",
    "* Return a new RDD with the specified number of partitions, \n",
    "* placing original items into the partition returned by a user supplied function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a58ecf1-7e59-4347-935b-67f3fa4f656f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('J', 'James'), ('F', 'Fred')], [('A', 'Anna'), ('J', 'John')], [('R', 'Ravi'), ('E', 'Eswar'), ('T', 'Tagore'), ('F', 'FSDF')]]\n[[('F', 'Fred'), ('A', 'Anna'), ('E', 'Eswar'), ('F', 'FSDF')], [('J', 'James'), ('J', 'John'), ('R', 'Ravi'), ('T', 'Tagore')]]\nX RDD No.OF Partitiones :  3\nY RDD No.OF Partitiones :  2\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('J','James'),('F','Fred'),('A','Anna'),('J','John'),('R','Ravi'),('E','Eswar'),('T','Tagore'),('F','FSDF')], 3)\n",
    "y = x.partitionBy(2, lambda w: 0 if w[0] < 'H' else 1)\n",
    "print (x.glom().collect())\n",
    "print (y.glom().collect())\n",
    "print('X RDD No.OF Partitiones : ',x.getNumPartitions())\n",
    "print('Y RDD No.OF Partitiones : ',y.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33690125-f10e-4bc9-8260-bb4015bbe8a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### ZIP Transformation\n",
    "* Return a new RDD containing pairs whose key is the item in the original RDD, and whose\n",
    "* value is that item’s corresponding element (same partition, same index) in a second RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e0aec0-d02f-4847-9f55-deca7e403598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 6), (3, 8), (4, 10), (5, 12), (6, 6)]\n"
     ]
    }
   ],
   "source": [
    "x= sc.parallelize([1,2,3,4,5,6])\n",
    "y = sc.parallelize([4,6,8,10,12,6])\n",
    "z = x.zip(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b954f43c-1c54-4e5a-80b1-cbb75e2b5a74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display X RDD Values:  [1, 2, 3]\nDisplay Y RDD Values:  [1, 4, 9]\n[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3])\n",
    "y = x.map(lambda n:n*n)\n",
    "z = x.zip(y)\n",
    "print('display X RDD Values: ',x.collect())\n",
    "print('Display Y RDD Values: ',y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "070b2103-85a0-4115-b017-a0abe72b42ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (2, 4), (3, 5), (5, 4), (6, 8)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3,5,6])\n",
    "y = sc.parallelize([3,4,5,4,8])\n",
    "z = x.zip(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a20223-923e-447f-8332-65c46a8bf39b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### groupByKey Transformation in RDD\n",
    "* When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\n",
    "* Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.\n",
    "* Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cef321e-0bb4-4614-8c84-c5b17e2ed901",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1), ('C', 4), ('C', 5), ('B', 4)]\n[('B', [5, 4, 4]), ('C', [4, 5]), ('A', [3, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1),('C',4),('C',5),('B',4)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0120a0e2-a89e-433c-8168-fdaee26e6aad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### JOINS\n",
    "* join(self, other, numPartitions=None)\n",
    "* Return an RDD containing all pairs of elements with matching keys in self and other.\n",
    "* Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other.\n",
    "* Performs a hash join across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428144b6-fe2c-4b34-b4ec-db42c64275b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 2)), ('a', (1, 3))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "z = x.join(y)\n",
    "print(z.collect())\n",
    "#print(sorted(z.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c279c16-846d-4be5-a866-dd2581de5f24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### leftOuterJoin(self, other, numPartitions=None)\n",
    " \n",
    "* Perform a left outer join of self and other.\n",
    "* For each element (k, v) in self, the resulting RDD will either contain all pairs (k, (v, w)) for w in other, or the pair (k, (v, None)) if no elements in other have key k.\n",
    "* Hash-partitions the resulting RDD into the given number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fde476f-b452-46c1-b17f-8d8b2dcd9e9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (4, None)), ('a', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "z = x.leftOuterJoin(y)\n",
    "print(z.collect())\n",
    "#print(sorted(z.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f30e010d-3718-4cf5-bd4f-8e357c127655",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### rightOuterJoin(self, other, numPartitions=None)\n",
    "* Perform a right outer join of self and other.\n",
    "* For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w)) if no elements in self have key k.\n",
    "* Hash-partitions the resulting RDD into the given number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d2f5bc-b55e-471b-ad38-0346edf520be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', (None, 4)), ('a', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1)])\n",
    "y = sc.parallelize([(\"a\", 2),(\"c\", 4)])\n",
    "z = x.rightOuterJoin(y)\n",
    "print(z.collect())\n",
    "#print(sorted(z.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b747dc6-144e-4908-9c82-f741bea137b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### fullOuterJoin(self, other, numPartitions=None)\n",
    "* Perform a full outer join of self and other and it will return both matching and unmatching result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8d672c-82c4-4dcf-9f93-94b2a40148c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (4, None)), ('c', (None, 5)), ('a', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"c\", 5)])\n",
    "z = x.fullOuterJoin(y)\n",
    "print(z.collect())\n",
    "#print(sorted(z.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e86c2f1-c0dd-4d9a-872d-75df2cdd7f13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### cogroup(self, other, numPartitions=None)\n",
    "* For each key k in self or other, return a resulting RDD that contains a tuple with the list of values for that key in self as well as other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bcfcb0-95fe-4dab-9cb4-eadbee94835c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[63]: [('b', [[4], []]), ('a', [[1], [2]])]"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "z = x.cogroup(y)\n",
    "[(x, list(map(list,y))) for x, y in z.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78b5e27-e833-40ae-8357-b674c14e7c73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is glom()\n",
    "* flattens elements on the same partition\n",
    "* glom() Return an RDD created by coalescing all elements within each partition into an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "975dc17a-4f06-40db-9ebe-a855293ebb2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Shuffle\n",
    "* The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s map and reduce operations.\n",
    "\n",
    "* Operations which can cause a shuffle include `repartition` operations like `repartition` and `coalesce`, ‘`ByKey` operations (except for counting) like `groupByKey` and `reduceByKey`, and `join` operations like cogroup and join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7c0324a-9afe-4eb2-a11c-89bf8911024b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### GetNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1099f609-d91b-47df-9375-74109d5f33ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 12, 122], [134, 1456, 1212, 121212, 98]]\n5\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 122, 134, 1456, 1212, 121212, 98]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,12,122,134,1456,1212,121212,98], 5)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)\n",
    "print(x.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d0bf0f-c0a6-4106-8eea-10a18c0712e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### reduceByKey(func, [numPartitions])\t\n",
    "* When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "\n",
    "* NOTE: Note If you are grouping using (groupbykey) in order to perform an aggregation (such as a sum or average) over each key, using `reduceByKey` or `aggregateByKey` will provide much better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c576661-f00e-411c-bcb6-47a68333050d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\nCPU times: user 30.2 ms, sys: 4.04 ms, total: 34.2 ms\nWall time: 565 ms\nOut[65]: [('cat', 2), ('elephant', 1), ('rat', 2)]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wordsRDD = sc.parallelize(['cat', 'elephant', 'rat', 'rat', 'cat'], 4)\n",
    "split_rdd = wordsRDD.map(lambda w:(w,1))\n",
    "result = split_rdd.reduceByKey(lambda x,y:x+y).collect()\n",
    "print(split_rdd.collect())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b396b77a-c6b7-4bd8-9f89-8d3a4babc1d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[66]: list"
     ]
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fa9eb2-c948-4807-a31b-44db29acee4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'elephant', 'rat', 'rat', 'cat']\n[('cat', 2), ('elephant', 1), ('rat', 2)]\nCPU times: user 11.1 ms, sys: 14.8 ms, total: 25.9 ms\nWall time: 438 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "wordCountsCollected = (wordsRDD.map(lambda w: (w, 1)).reduceByKey(lambda x,y: x+y).collect())\n",
    "print(wordsRDD.collect())\n",
    "print(wordCountsCollected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e09a8f09-8c0c-4b4b-a7e6-68cd561bd172",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe701046-16d4-44b6-8615-cb05153695e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### reduce(func)  Action\n",
    "* Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). \n",
    "* The function should be commutative and associative so that it can be computed correctly in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a678bf7-6fb4-4bc2-a3ce-ab1afe32ee19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n55\nOut[68]: int"
     ]
    }
   ],
   "source": [
    "# reduce numbers 1 to 10 by adding them up\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "y = x.reduce(lambda a,b: a+b)\n",
    "print(x.collect())\n",
    "print(y)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f926c8cf-4916-4326-a90c-7cb0dd297fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### count()\n",
    "* Return the number of elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a076fd5e-7934-496c-96f9-0e280bf96eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[70]: 18"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,34,34,34,34,6,45,23,2,0])\n",
    "x.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64a604a-37fb-4720-abe9-c64e4a1e03be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### first() Action\n",
    "* Return the first element of the dataset (similar to take(1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd57bf5-75c7-4e40-81cf-261a155b9389",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n[66, 77, 55, 44]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([66,77,55,44,33,1,2,3,4])\n",
    "print(x.first())\n",
    "print(x.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481e60cb-0f4c-4311-98f2-3cea6a60900b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### takeSample(withReplacement, num, [seed])\n",
    "* Return an array with a random sample of num elements of the dataset, with or without replacement,\n",
    "* optionally pre-specifying a random number generator seed.\n",
    "* Return a fixed-size sampled subset of this RDD \n",
    "* withReplacement whether sampling is done with replacement\n",
    "* num             size of the returned sample\n",
    "* seed            seed for the random number generator\n",
    "* returns         sample of specified size in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681904c1-5564-435c-8f24-40e3abb25040",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 1, 6, 3, 8, 3, 9, 8, 2, 2, 5, 4, 4, 3, 6, 3, 3, 0, 0, 0, 4, 1, 1, 9, 7, 3, 7, 1, 5, 0, 1, 6, 4, 0, 9, 9, 0, 6, 5, 8, 1, 1, 5, 4, 2, 6, 1, 2]\n[6, 8, 9, 7, 5, 3, 0, 4, 1, 2]\n[5, 9, 3, 4, 6]\n[2, 3, 1, 0, 8, 7, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(0, 10))\n",
    "print(rdd.takeSample(True, 50, 3))\n",
    "print(rdd.takeSample(False, 20, 1))\n",
    "\n",
    "print(rdd.takeSample(False, 5, 2))\n",
    "\n",
    "print(rdd.takeSample(False, 8, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110404b7-1763-46de-8c65-516083a5d3b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### take(n) Action\n",
    "* Return an array with the first n elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad53845-9554-4eae-bfac-c67a99069bf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take Action :  [1, 2]\nNo of records : 4\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4])\n",
    "print('Take Action : ',x.take(2))\n",
    "print('No of records :',x.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f831d600-522f-49b4-9b6d-0d34415a1c46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### countByValue(self)\n",
    "* Return the count of each unique value in this RDD as a dictionary of (value, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990d91c5-77a0-42ff-8b2e-e2000dc9b974",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[96]: set"
     ]
    }
   ],
   "source": [
    "a={44,23,23,23}\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65124ded-a1b2-4f1a-9e87-aa0939a4d5ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(1, 2), (2, 3), (3, 4), (4, 3), (5, 2), (6, 2)])\nOut[75]: dict_items"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 1, 2, 2,3,3,4,3,3,4,4,5,5,6,6])\n",
    "y = x.countByValue().items()\n",
    "print(y)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e49e95-dde5-4511-bbc3-571bd7457f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### isEmpty()\n",
    "* Returns true if and only if the RDD contains no elements at all.\n",
    "* note:: an RDD may be empty even when it has at least 1 partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25339c5f-7bc6-4809-b392-b7f7b9f314c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFalse\n[]\nTrue\n"
     ]
    }
   ],
   "source": [
    "x =  sc.parallelize(range(10))\n",
    "print(x.collect())\n",
    "print(x.isEmpty())\n",
    "y =  sc.parallelize(range(0))\n",
    "print(y.collect())\n",
    "print(y.isEmpty())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1213c7e-d869-4c68-a9d1-3c001d552613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### keys()\n",
    "* Return an RDD with the keys of each tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8b6ebd-665f-4da7-bc8b-31d0c850c932",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(1, 2), (3, 4),(5,55),(6,66)])\n",
    "y = x.keys()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813cd19d-35d8-40d4-8031-aabfb7950050",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### saveAsTextFile(path, compressionCodecClass=None)\n",
    "* Save the RDD to the filesystem indicated in the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0070c5ff-0756-4aa1-91b2-e4e823d5d278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[78]: False"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/tmp/test_data/\",True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d57e1ec-a3a7-4098-b408-d7e2f9e79ed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd_text = sc.textFile('dbfs:/tmp/test_data/saveAs/part-00007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79b2f83-8c5c-4166-a731-b9eee11db9dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3737989515766202>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mrdd_text\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/rdd.py:1825\u001B[0m, in \u001B[0;36mRDD.collect\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1823\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext):\n",
       "\u001B[1;32m   1824\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 1825\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1826\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
       ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: dbfs:/tmp/test_data/saveAs/part-00007\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
       "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:223)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1069)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1068)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n",
       "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
       "\tat sun.reflect.GeneratedMethodAccessor496.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.io.IOException: Input path does not exist: dbfs:/tmp/test_data/saveAs/part-00007\n",
       "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n",
       "\t... 29 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-3737989515766202>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrdd_text\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/rdd.py:1825\u001B[0m, in \u001B[0;36mRDD.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1823\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext):\n\u001B[1;32m   1824\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1825\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1826\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: dbfs:/tmp/test_data/saveAs/part-00007\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:223)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1069)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1068)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor496.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Input path does not exist: dbfs:/tmp/test_data/saveAs/part-00007\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 29 more\n",
       "errorSummary": "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: dbfs:/tmp/test_data/saveAs/part-00007",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4ea646-7def-4055-87da-9bb530111632",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:1)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:43)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:45)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:47)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw.&lt;init&gt;(command-3737989515766203:49)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw.&lt;init&gt;(command-3737989515766203:51)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read.&lt;init&gt;(command-3737989515766203:53)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$.&lt;init&gt;(command-3737989515766203:57)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$read$.&lt;clinit&gt;(command-3737989515766203)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $line4a0a5add275142699679641856db5cbf25.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1298)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1251)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$25(DriverLocal.scala:904)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:125)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:895)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:872)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:719)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:711)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:739)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:628)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:663)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:499)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:438)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:262)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:1)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:43)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:45)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw$$iw.&lt;init&gt;(command-3737989515766203:47)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw$$iw.&lt;init&gt;(command-3737989515766203:49)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$$iw.&lt;init&gt;(command-3737989515766203:51)\n\tat $line4a0a5add275142699679641856db5cbf25.$read.&lt;init&gt;(command-3737989515766203:53)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$.&lt;init&gt;(command-3737989515766203:57)\n\tat $line4a0a5add275142699679641856db5cbf25.$read$.&lt;clinit&gt;(command-3737989515766203)\n\tat $line4a0a5add275142699679641856db5cbf25.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $line4a0a5add275142699679641856db5cbf25.$eval$.$print(&lt;notebook&gt;:6)\n\tat $line4a0a5add275142699679641856db5cbf25.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1298)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1251)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$25(DriverLocal.scala:904)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:125)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:895)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:872)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:719)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:711)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:739)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:628)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:663)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:499)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:262)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "FileNotFoundException: /tmp/test_data/saveAs",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/tmp/test_data/saveAs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c896c3-adcd-47f8-9aa7-3e62ce6bf840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize(['Raveendra','Eswar','Vamsi','Lakshmi','Vinod'])\n",
    "x.saveAsTextFile(\"dbfs:/tmp/test_data/saveAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f730c7f-6a18-4af6-a9c4-2ab49520eece",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/test_data/saveAs/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00000</td><td>part-00000</td><td>0</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00001</td><td>part-00001</td><td>10</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00002</td><td>part-00002</td><td>0</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00003</td><td>part-00003</td><td>6</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00004</td><td>part-00004</td><td>6</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00005</td><td>part-00005</td><td>0</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00006</td><td>part-00006</td><td>8</td><td>1718288990000</td></tr><tr><td>dbfs:/tmp/test_data/saveAs/part-00007</td><td>part-00007</td><td>6</td><td>1718288990000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/tmp/test_data/saveAs/_SUCCESS",
         "_SUCCESS",
         0,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00000",
         "part-00000",
         0,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00001",
         "part-00001",
         10,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00002",
         "part-00002",
         0,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00003",
         "part-00003",
         6,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00004",
         "part-00004",
         6,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00005",
         "part-00005",
         0,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00006",
         "part-00006",
         8,
         1718288990000
        ],
        [
         "dbfs:/tmp/test_data/saveAs/part-00007",
         "part-00007",
         6,
         1718288990000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/tmp/test_data/saveAs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6ad946-27fe-4233-8a56-b5565818b6a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Raveendra', 'Eswar', 'Vamsi', 'Lakshmi', 'Vinod']\n"
     ]
    }
   ],
   "source": [
    "y = sc.textFile(\"dbfs:/tmp/test_data/saveAs\")\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c8340d-ef4d-4393-9626-d20ca06cab30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### saveAsPickleFile(self, path, batchSize=10)\n",
    "* Save this RDD as a SequenceFile of serialized objects. The serializer\n",
    "* used is :class:`pyspark.serializers.PickleSerializer`, default batch size is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985fc71d-cb96-44e8-9123-945e79beca01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[83]: False"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/tmp/test_data/picklefile/\",True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f71a1b3-82b8-4006-8e7e-4bb5197254d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize(['Raveendra','Eswar','Vinod','Lakshmi','Vamsi'])\n",
    "x.saveAsPickleFile(\"dbfs:/tmp/test_data/picklefile\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c270620b-297a-4956-911e-5f2bae801e94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/test_data/picklefile/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00000</td><td>part-00000</td><td>95</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00001</td><td>part-00001</td><td>185</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00002</td><td>part-00002</td><td>95</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00003</td><td>part-00003</td><td>181</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00004</td><td>part-00004</td><td>181</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00005</td><td>part-00005</td><td>95</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00006</td><td>part-00006</td><td>183</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00007</td><td>part-00007</td><td>181</td><td>1718288995000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/tmp/test_data/picklefile/_SUCCESS",
         "_SUCCESS",
         0,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00000",
         "part-00000",
         95,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00001",
         "part-00001",
         185,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00002",
         "part-00002",
         95,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00003",
         "part-00003",
         181,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00004",
         "part-00004",
         181,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00005",
         "part-00005",
         95,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00006",
         "part-00006",
         183,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00007",
         "part-00007",
         181,
         1718288995000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/tmp/test_data/picklefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79570f5e-5df8-4ea5-a5d8-197e08936b18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[85]: ['SEQ\\x06!org.apache.hadoop.io.NullWritable\"org.apache.hadoop.io.BytesWritable\\x00\\x00\\x00\\x00\\x00\\x00U�\\x197Śl�$|��H��G\\x00\\x00\\x00N\\x00\\x00\\x00\\x00\\x00\\x00\\x00J��\\x00\\x05ur\\x00\\x03[[BK�\\x19\\x15gg�7\\x02\\x00\\x00xp\\x00\\x00\\x00\\x01ur\\x00\\x02[B��\\x17�\\x06\\x08T�\\x02\\x00\\x00xp\\x00\\x00\\x00\\x17�\\x05�\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00]��\\x05Vinod�a.']"
     ]
    }
   ],
   "source": [
    "text_rdd = sc.textFile('dbfs:/tmp/test_data/picklefile/part-00004')\n",
    "text_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a57ede-742e-43c0-afe3-9d570d1849bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Raveendra', 'Eswar', 'Vinod', 'Lakshmi', 'Vamsi']\n"
     ]
    }
   ],
   "source": [
    "y = sc.pickleFile(\"dbfs:/tmp/test_data/picklefile\")\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f31e3289-32ce-45fd-8a81-17e8733995b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/test_data/picklefile/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00000</td><td>part-00000</td><td>95</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00001</td><td>part-00001</td><td>185</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00002</td><td>part-00002</td><td>95</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00003</td><td>part-00003</td><td>181</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00004</td><td>part-00004</td><td>181</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00005</td><td>part-00005</td><td>95</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00006</td><td>part-00006</td><td>183</td><td>1718288995000</td></tr><tr><td>dbfs:/tmp/test_data/picklefile/part-00007</td><td>part-00007</td><td>181</td><td>1718288995000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/tmp/test_data/picklefile/_SUCCESS",
         "_SUCCESS",
         0,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00000",
         "part-00000",
         95,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00001",
         "part-00001",
         185,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00002",
         "part-00002",
         95,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00003",
         "part-00003",
         181,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00004",
         "part-00004",
         181,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00005",
         "part-00005",
         95,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00006",
         "part-00006",
         183,
         1718288995000
        ],
        [
         "dbfs:/tmp/test_data/picklefile/part-00007",
         "part-00007",
         181,
         1718288995000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/tmp/test_data/picklefile/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efd2c68-eccd-4321-9442-3df5e7f16b63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### STDEV()\n",
    "* Return the standard deviation of the items in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f051a372-3868-401e-82c8-1aa65c64cb1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n1.247219128924647\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2,4,1])\n",
    "y = x.stdev()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad308f1f-1894-4cdf-8476-67c9c2bd0779",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### MIN()\n",
    "* Return the MIN value of the items in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecee42f-a611-4b07-9530-66b3645fa84c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1, 3, 5, 6, 7]\n1\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2,4,1,3,5,6,7])\n",
    "y = x.min()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf8e8fd-c10c-48b4-9990-30944ab06948",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### MAX()\n",
    "* REturn the MAX Value of the items in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47c11ae-a123-4153-812b-787b00a7134f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1, 55, 66, 77, 8845, 454545]\n454545\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2,4,1,55,66,77,8845,454545,])\n",
    "y = x.max()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c1b5db-1556-4e97-bca5-20848e2770a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### MEAN()\n",
    "* Return the mean of the items in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa7e17f-5a70-4316-8aff-63e8c991ca14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n2.3333333333333335\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2,4,1])\n",
    "y = x.mean()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb9d301-d1e3-4c21-aab0-bc2e06ff8841",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SUM()\n",
    "* Return the Sum of the items in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba2d6de-9d3c-4577-ac45-ed20e918825d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1, 55, 44, 34, 34, 344]\n518\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2,4,1,55,44,34,34,344])\n",
    "y = x.sum()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf398f4-0ddf-43a6-a9e4-62ecd16230cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Stats()\n",
    "* Return a StatCounter object that captures the mean, variance and count of the RDD’s elements in one operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27feb134-bbe1-4c0e-91a2-7aef73152b04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[92]: (count: 15, mean: 8.0, stdev: 4.320493798938574, max: 15.0, min: 1.0)"
     ]
    }
   ],
   "source": [
    "list_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "list_rdd.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746c6d52-55a0-464c-84f8-9ca564daa5fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_rdd = sc.parallelize(['test1','test2','test3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b88edfc-e513-47cf-8c38-f9dbb73f34cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n\nclass RDD(typing.Generic)\n |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |  \n |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n |  Represents an immutable, partitioned collection of elements that can be\n |  operated on in parallel.\n |  \n |  Method resolution order:\n |      RDD\n |      typing.Generic\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> (rdd + rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  __getnewargs__(self) -> NoReturn\n |  \n |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given combine functions and a neutral \"zero\n |      value.\"\n |      \n |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      The first function (seqOp) can return a different result type, U, than\n |      the type of this RDD. Thus, we need one operation for merging a T into\n |      an U and one operation for merging two U\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : U\n |          the initial value for the accumulated result of each partition\n |      seqOp : function\n |          a function used to accumulate results within a partition\n |      combOp : function\n |          an associative function used to combine results from different partitions\n |      \n |      Returns\n |      -------\n |      U\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduce`\n |      :meth:`RDD.fold`\n |      \n |      Examples\n |      --------\n |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n |      (10, 4)\n |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n |      (0, 0)\n |  \n |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f13c35b0af0>) -> 'RDD[Tuple[K, U]]'\n |      Aggregate the values of each key, using given combine functions and a neutral\n |      \"zero value\". This function can return a different result type, U, than the type\n |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n |      a U and one operation for merging two U's, The former operation is used for merging\n |      values within a partition, and the latter is used for merging values between\n |      partitions. To avoid memory allocation, both of these functions are\n |      allowed to modify and return their first argument instead of creating a new U.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : U\n |          the initial value for the accumulated result of each partition\n |      seqFunc : function\n |          a function to merge a V into a U\n |      combFunc : function\n |          a function to combine two U's into a single one\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      partitionFunc : function, optional, default `portable_hash`\n |          function to compute the partition index\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and the aggregated result for each key\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduceByKey`\n |      :meth:`RDD.combineByKey`\n |      :meth:`RDD.foldByKey`\n |      :meth:`RDD.groupByKey`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n |      >>> seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n |      >>> combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n |      >>> sorted(rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect())\n |      [('a', (3, 2)), ('b', (1, 1))]\n |  \n |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n |      entire stage and relaunch all tasks for this stage.\n |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Returns\n |      -------\n |      :class:`RDDBarrier`\n |          instance that provides actions within a barrier stage.\n |      \n |      See Also\n |      --------\n |      :class:`pyspark.BarrierTaskContext`\n |      \n |      Notes\n |      -----\n |      For additional information see\n |      \n |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n |      \n |      This API is experimental\n |  \n |  cache(self: 'RDD[T]') -> 'RDD[T]'\n |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          The same :class:`RDD` with storage level set to `MEMORY_ONLY`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.persist`\n |      :meth:`RDD.unpersist`\n |      :meth:`RDD.getStorageLevel`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd2 = rdd.cache()\n |      >>> rdd2 is rdd\n |      True\n |      >>> str(rdd.getStorageLevel())\n |      'Memory Serialized 1x Replicated'\n |      >>> _ = rdd.unpersist()\n |  \n |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Return the Cartesian product of this RDD and another one, that is, the\n |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n |      ``b`` is in `other`.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the Cartesian product of this :class:`RDD` and another one\n |      \n |      See Also\n |      --------\n |      :meth:`pyspark.sql.DataFrame.crossJoin`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2])\n |      >>> sorted(rdd.cartesian(rdd).collect())\n |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n |  \n |  checkpoint(self) -> None\n |      Mark this RDD for checkpointing. It will be saved to a file inside the\n |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n |      all references to its parent RDDs will be removed. This function must\n |      be called before any job has been executed on this RDD. It is strongly\n |      recommended that this RDD is persisted in memory, otherwise saving it\n |      on a file will require recomputation.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.isCheckpointed`\n |      :meth:`RDD.getCheckpointFile`\n |      :meth:`RDD.localCheckpoint`\n |      :meth:`SparkContext.setCheckpointDir`\n |      :meth:`SparkContext.getCheckpointDir`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.is_checkpointed\n |      False\n |      >>> rdd.getCheckpointFile() == None\n |      True\n |      \n |      >>> rdd.checkpoint()\n |      >>> rdd.is_checkpointed\n |      True\n |      >>> rdd.getCheckpointFile() == None\n |      True\n |      \n |      >>> rdd.count()\n |      5\n |      >>> rdd.is_checkpointed\n |      True\n |      >>> rdd.getCheckpointFile() == None\n |      False\n |  \n |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n |      Removes an RDD's shuffles and it's non-persisted ancestors.\n |      \n |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n |      If you use the RDD after this call, you should checkpoint and materialize it first.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional, default False\n |         whether to block on shuffle cleanup tasks\n |      \n |      Notes\n |      -----\n |      This API is a developer API.\n |  \n |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n |      Return a new RDD that is reduced into `numPartitions` partitions.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      shuffle : bool, optional, default False\n |          whether to add a shuffle step\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` that is reduced into `numPartitions` partitions\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.repartition`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n |      [[1], [2, 3], [4, 5]]\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n |      [[1, 2, 3, 4, 5]]\n |  \n |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n |      For each key k in `self` or `other`, return a resulting RDD that\n |      contains a tuple with the list of values for that key in `self` as\n |      well as `other`.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and cogrouped values\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.groupWith`\n |      :meth:`RDD.join`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n |      [('a', ([1], [2])), ('b', ([4], []))]\n |  \n |  collect(self: 'RDD[T]') -> List[~T]\n |      Return a list that contains all the elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      list\n |          a list containing all the elements\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.toLocalIterator`\n |      :meth:`pyspark.sql.DataFrame.collect`\n |      \n |      Examples\n |      --------\n |      >>> sc.range(5).collect()\n |      [0, 1, 2, 3, 4]\n |      >>> sc.parallelize([\"x\", \"y\", \"z\"]).collect()\n |      ['x', 'y', 'z']\n |  \n |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n |      Return the key-value pairs in this RDD to the master as a dictionary.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`dict`\n |          a dictionary of (key, value) pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.countByValue`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting data is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n |      >>> m[1]\n |      2\n |      >>> m[3]\n |      4\n |  \n |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n |      When collect rdd, use this method to specify job group.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      .. deprecated:: 3.1.0\n |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n |      \n |      Parameters\n |      ----------\n |      groupId : str\n |          The group ID to assign.\n |      description : str\n |          The description to set for the job group.\n |      interruptOnCancel : bool, optional, default False\n |          whether to interrupt jobs on job cancellation.\n |      \n |      Returns\n |      -------\n |      list\n |          a list containing all the elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collect`\n |      :meth:`SparkContext.setJobGroup`\n |  \n |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f13c35b0af0>) -> 'RDD[Tuple[K, U]]'\n |      Generic function to combine the elements for each key using a custom\n |      set of aggregation functions.\n |      \n |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n |      type\" C.\n |      \n |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n |      modify and return their first argument instead of creating a new C.\n |      \n |      In addition, users can control the partitioning of the output RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      createCombiner : function\n |          a function to turns a V into a C\n |      mergeValue : function\n |          a function to merge a V into a C\n |      mergeCombiners : function\n |          a function to combine two C's into a single one\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      partitionFunc : function, optional, default `portable_hash`\n |          function to compute the partition index\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and the aggregated result for each key\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduceByKey`\n |      :meth:`RDD.aggregateByKey`\n |      :meth:`RDD.foldByKey`\n |      :meth:`RDD.groupByKey`\n |      \n |      Notes\n |      -----\n |      V and C can be different -- for example, one might group an RDD of type\n |          (Int, Int) into an RDD of type (Int, List[Int]).\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n |      >>> def to_list(a):\n |      ...     return [a]\n |      ...\n |      >>> def append(a, b):\n |      ...     a.append(b)\n |      ...     return a\n |      ...\n |      >>> def extend(a, b):\n |      ...     a.extend(b)\n |      ...     return a\n |      ...\n |      >>> sorted(rdd.combineByKey(to_list, append, extend).collect())\n |      [('a', [1, 2]), ('b', [1])]\n |  \n |  count(self) -> int\n |      Return the number of elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      int\n |          the number of elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.countApprox`\n |      :meth:`pyspark.sql.DataFrame.count`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).count()\n |      3\n |  \n |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n |      Approximate version of count() that returns a potentially incomplete\n |      result within a timeout, even if not all tasks have finished.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Parameters\n |      ----------\n |      timeout : int\n |          maximum time to wait for the job, in milliseconds\n |      confidence : float\n |          the desired statistical confidence in the result\n |      \n |      Returns\n |      -------\n |      int\n |          a potentially incomplete result, with error bounds\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.count`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> rdd.countApprox(1000, 1.0)\n |      1000\n |  \n |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n |      Return approximate number of distinct elements in the RDD.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Parameters\n |      ----------\n |      relativeSD : float, optional\n |          Relative accuracy. Smaller values create\n |          counters that require more space.\n |          It must be greater than 0.000017.\n |      \n |      Returns\n |      -------\n |      int\n |          approximate number of distinct elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.distinct`\n |      \n |      Notes\n |      -----\n |      The algorithm used is based on streamlib's implementation of\n |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n |      of The Art Cardinality Estimation Algorithm\", available here\n |      <https://doi.org/10.1145/2452376.2452456>`_.\n |      \n |      Examples\n |      --------\n |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n |      >>> 900 < n < 1100\n |      True\n |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n |      >>> 16 < n < 24\n |      True\n |  \n |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n |      Count the number of elements for each key, and return the result to the\n |      master as a dictionary.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      dict\n |          a dictionary of (key, count) pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collectAsMap`\n |      :meth:`RDD.countByValue`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> sorted(rdd.countByKey().items())\n |      [('a', 2), ('b', 1)]\n |  \n |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n |      Return the count of each unique value in this RDD as a dictionary of\n |      (value, count) pairs.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      dict\n |          a dictionary of (value, count) pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collectAsMap`\n |      :meth:`RDD.countByKey`\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n |      [(1, 2), (2, 3)]\n |  \n |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return a new RDD containing the distinct elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD` containing the distinct elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.countApproxDistinct`\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n |      [1, 2, 3]\n |  \n |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n |      Return a new RDD containing only the elements that satisfy a predicate.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |          a function to run on each element of the RDD\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD` by applying a function to each element\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.map`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n |      [2, 4]\n |  \n |  first(self: 'RDD[T]') -> ~T\n |      Return the first element in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      T\n |          the first element\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.take`\n |      :meth:`pyspark.sql.DataFrame.first`\n |      :meth:`pyspark.sql.DataFrame.head`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).first()\n |      2\n |      >>> sc.parallelize([]).first()\n |      Traceback (most recent call last):\n |          ...\n |      ValueError: RDD is empty\n |  \n |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n |      Return a new RDD by first applying a function to all elements of this\n |      RDD, and then flattening the results.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |          a function to turn a T into a sequence of U\n |      preservesPartitioning : bool, optional, default False\n |          indicates whether the input function preserves the partitioner,\n |          which should be False unless this is a pair RDD and the input\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD` by applying a function to all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.map`\n |      :meth:`RDD.mapPartitions`\n |      :meth:`RDD.mapPartitionsWithIndex`\n |      :meth:`RDD.mapPartitionsWithSplit`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([2, 3, 4])\n |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n |      [1, 1, 1, 2, 2, 3]\n |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n |  \n |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n |      Pass each value in the key-value pair RDD through a flatMap function\n |      without changing the keys; this also retains the original RDD's\n |      partitioning.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |         a function to turn a V into a sequence of U\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and the flat-mapped value\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.flatMap`\n |      :meth:`RDD.mapValues`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n |      >>> def f(x): return x\n |      >>> rdd.flatMapValues(f).collect()\n |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n |  \n |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given associative function and a neutral \"zero value.\"\n |      \n |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      This behaves somewhat differently from fold operations implemented\n |      for non-distributed collections in functional languages like Scala.\n |      This fold operation may be applied to partitions individually, and then\n |      fold those results into the final result, rather than apply the fold\n |      to each element sequentially in some defined ordering. For functions\n |      that are not commutative, the result may differ from that of a fold\n |      applied to a non-distributed collection.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : T\n |          the initial value for the accumulated result of each partition\n |      op : function\n |          a function used to both accumulate results within a partition and combine\n |          results from different partitions\n |      \n |      Returns\n |      -------\n |      T\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduce`\n |      :meth:`RDD.aggregate`\n |      \n |      Examples\n |      --------\n |      >>> from operator import add\n |      >>> sc\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n r in result])\n |      'bar\\nfoo\\n'\n |  \n |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n |      Assign a name to this RDD.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          new name\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the same :class:`RDD` with name updated\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.name`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2])\n |      >>> rdd.setName('I am an RDD').name()\n |      'I am an RDD'\n |  \n |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Sorts this RDD by the given keyfunc\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      keyfunc : function\n |          a function to compute the key\n |      ascending : bool, optional, default True\n |          sort the keys in ascending or descending order\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sortByKey`\n |      :meth:`pyspark.sql.DataFrame.sort`\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |  \n |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f13bb631670>) -> 'RDD[Tuple[K, V]]'\n |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      ascending : bool, optional, default True\n |          sort the keys in ascending or descending order\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      keyfunc : function, optional, default identity mapping\n |          a function to compute the key\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sortBy`\n |      :meth:`pyspark.sql.DataFrame.sort`\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortByKey().first()\n |      ('1', 3)\n |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n |  \n |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n |      Return a :class:`StatCounter` object that captures the mean, variance\n |      and count of the RDD's elements in one operation.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Returns\n |      -------\n |      :class:`StatCounter`\n |          a :class:`StatCounter` capturing the mean, variance and count of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.stdev`\n |      :meth:`RDD.sampleStdev`\n |      :meth:`RDD.variance`\n |      :meth:`RDD.sampleVariance`\n |      :meth:`RDD.histogram`\n |      :meth:`pyspark.sql.DataFrame.stat`\n |  \n |  stdev(self: 'RDD[NumberOrArray]') -> float\n |      Compute the standard deviation of this RDD's elements.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Returns\n |      -------\n |      float\n |          the standard deviation of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.stats`\n |      :meth:`RDD.sampleStdev`\n |      :meth:`RDD.variance`\n |      :meth:`RDD.sampleVariance`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).stdev()\n |      0.816...\n |  \n |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return each value in `self` that is not contained in `other`.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` with the elements from this that are not in `other`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.subtractByKey`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(rdd1.subtract(rdd2).collect())\n |      [('a', 1), ('b', 4), ('b', 5)]\n |  \n |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n |      Return each (key, value) pair in `self` that has no pair with matching\n |      key in `other`.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` with the pairs from this whose keys are not in `other`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.subtract`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(rdd1.subtractByKey(rdd2).collect())\n |      [('b', 4), ('b', 5)]\n |  \n |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Add up the elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      float, int, or complex\n |          the sum of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.mean`\n |      :meth:`RDD.sumApprox`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n |      6.0\n |  \n |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n |      Approximate operation to return the sum within a timeout\n |      or meet the confidence.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Parameters\n |      ----------\n |      timeout : int\n |          maximum time to wait for the job, in milliseconds\n |      confidence : float\n |          the desired statistical confidence in the result\n |      \n |      Returns\n |      -------\n |      :class:`BoundedFloat`\n |          a potentially incomplete result, with error bounds\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sum`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> r = sum(range(1000))\n |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n |      True\n |  \n |  take(self: 'RDD[T]', num: int) -> List[~T]\n |      Take the first num elements of the RDD.\n |      \n |      It works by first scanning one partition, and use the results from\n |      that partition to estimate the number of additional partitions needed\n |      to satisfy the limit.\n |      \n |      Translated from the Scala implementation in RDD#take().\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      num : int\n |          first number of elements\n |      \n |      Returns\n |      -------\n |      list\n |          the first `num` elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.first`\n |      :meth:`pyspark.sql.DataFrame.take`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n |      [2, 3]\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n |      [2, 3, 4, 5, 6]\n |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n |      [91, 92, 93]\n |  \n |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the N elements from an RDD ordered in ascending order or as\n |      specified by the optional key function.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      num : int\n |          top N\n |      key : function, optional\n |          a function used to generate key for comparing\n |      \n |      Returns\n |      -------\n |      list\n |          the top N elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.top`\n |      :meth:`RDD.max`\n |      :meth:`RDD.min`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n |      [1, 2, 3, 4, 5, 6]\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n |      [10, 9, 7, 6, 5, 4]\n |  \n |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n |      Return a fixed-size sampled subset of this RDD.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      withReplacement : list\n |          whether sampling is done with replacement\n |      num : int\n |          size of the returned sample\n |      seed : int, optional\n |          random seed\n |      \n |      Returns\n |      -------\n |      list\n |          a fixed-size sampled subset of this :class:`RDD` in an array\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sample`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(0, 10))\n |      >>> len(rdd.takeSample(True, 20, 1))\n |      20\n |      >>> len(rdd.takeSample(False, 5, 2))\n |      5\n |      >>> len(rdd.takeSample(False, 15, 3))\n |      10\n |  \n |  toDF(self, schema=None, sampleRatio=None)\n |      Converts current :class:`RDD` into a :class:`DataFrame`\n |      \n |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n |      \n |      Parameters\n |      ----------\n |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n |          column names, default is None.  The data type string format equals to\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n |      sampleRatio : float, optional\n |          the sample ratio of rows used for inferring\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      >>> rdd = spark.range(1).rdd.map(lambda x: tuple(x))\n |      >>> rdd.collect()\n |      [(0,)]\n |      >>> rdd.toDF().show()\n |      +---+\n |      | _1|\n |      +---+\n |      |  0|\n |      +---+\n |  \n |  toDebugString(self) -> Optional[bytes]\n |      A description of this RDD and its recursive dependencies for debugging.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Returns\n |      -------\n |      bytes\n |          debugging information of this :class:`RDD`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.toDebugString()\n |      b'...PythonRDD...ParallelCollectionRDD...'\n |  \n |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n |      Return an iterator that contains all of the elements in this RDD.\n |      The iterator will consume as much memory as the largest partition in this RDD.\n |      With prefetch it may consume up to the memory of the 2 largest partitions.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      prefetchPartitions : bool, optional\n |          If Spark should pre-fetch the next partition\n |          before it is needed.\n |      \n |      Returns\n |      -------\n |      :class:`collections.abc.Iterator`\n |          an iterator that contains all of the elements in this :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collect`\n |      :meth:`pyspark.sql.DataFrame.toLocalIterator`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(10))\n |      >>> [x for x in rdd.toLocalIterator()]\n |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |  \n |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the top N elements from an RDD.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      num : int\n |          top N\n |      key : function, optional\n |          a function used to generate key for comparing\n |      \n |      Returns\n |      -------\n |      list\n |          the top N elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.takeOrdered`\n |      :meth:`RDD.max`\n |      :meth:`RDD.min`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      It returns the list sorted in descending order.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n |      [12]\n |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n |      [6, 5]\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n |      [4, 3, 2]\n |  \n |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n |      Aggregates the elements of this RDD in a multi-level tree\n |      pattern.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : U\n |          the initial value for the accumulated result of each partition\n |      seqOp : function\n |          a function used to accumulate results within a partition\n |      combOp : function\n |          an associative function used to combine results from different partitions\n |      depth : int, optional, default 2\n |          suggested depth of the tree\n |      \n |      Returns\n |      -------\n |      U\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.aggregate`\n |      :meth:`RDD.treeReduce`\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeAggregate(0, add, add)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 1)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 2)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 5)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 10)\n |      -5\n |  \n |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n |      Reduces the elements of this RDD in a multi-level tree pattern.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |          the reduce function\n |      depth : int, optional, default 2\n |          suggested depth of the tree (default: 2)\n |      \n |      Returns\n |      -------\n |      T\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduce`\n |      :meth:`RDD.aggregate`\n |      :meth:`RDD.treeAggregate`\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeReduce(add)\n |      -5\n |      >>> rdd.treeReduce(add, 1)\n |      -5\n |      >>> rdd.treeReduce(add, 2)\n |      -5\n |      >>> rdd.treeReduce(add, 5)\n |      -5\n |      >>> rdd.treeReduce(add, 10)\n |      -5\n |  \n |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the union of this :class:`RDD` and another one\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.union`\n |      :meth:`pyspark.sql.DataFrame.union`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> rdd.union(rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n |      Mark the RDD as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional, default False\n |          whether to block until all blocks are deleted\n |      \n |          .. versionadded:: 3.0.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          The same :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.cache`\n |      :meth:`RDD.persist`\n |      :meth:`RDD.getStorageLevel`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.is_cached\n |      False\n |      >>> _ = rdd.unpersist()\n |      >>> rdd.is_cached\n |      False\n |      >>> _ = rdd.cache()\n |      >>> rdd.is_cached\n |      True\n |      >>> _ = rdd.unpersist()\n |      >>> rdd.is_cached\n |      False\n |      >>> _ = rdd.unpersist()\n |  \n |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n |      Return an RDD with the values of each tuple.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` only containing the values\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.keys`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n |      >>> rdd.collect()\n |      [2, 4]\n |  \n |  variance(self: 'RDD[NumberOrArray]') -> float\n |      Compute the variance of this RDD's elements.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Returns\n |      -------\n |      float\n |          the variance of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.stats`\n |      :meth:`RDD.sampleVariance`\n |      :meth:`RDD.stdev`\n |      :meth:`RDD.sampleStdev`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).variance()\n |      0.666...\n |  \n |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n |      This is only supported on certain cluster managers and currently requires dynamic\n |      allocation to be enabled. It will result in new executors with the resources specified\n |      being acquired to calculate the RDD.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Parameters\n |      ----------\n |      profile : :class:`pyspark.resource.ResourceProfile`\n |          a resource profile\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the same :class:`RDD` with user specified profile\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.getResourceProfile`\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Zips this RDD with another one, returning key-value pairs with the\n |      first element in each RDD second element in each RDD, etc. Assumes\n |      that the two RDDs have the same number of partitions and the same\n |      number of elements in each partition (e.g. one was made through\n |      a map on the other).\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the zipped key-value pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.zipWithIndex`\n |      :meth:`RDD.zipWithUniqueId`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize(range(0,5))\n |      >>> rdd2 = sc.parallelize(range(1000, 1005))\n |      >>> rdd1.zip(rdd2).collect()\n |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n |  \n |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with its element indices.\n |      \n |      The ordering is first based on the partition index and then the\n |      ordering of items within each partition. So the first item in\n |      the first partition gets index 0, and the last item in the last\n |      partition receives the largest index.\n |      \n |      This method needs to trigger a spark job when this RDD contains\n |      more than one partitions.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the zipped key-index pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.zip`\n |      :meth:`RDD.zipWithUniqueId`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n |  \n |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with generated unique Long ids.\n |      \n |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n |      n is the number of partitions. So there may exist gaps, but this\n |      method won't trigger a spark job, which is different from\n |      :meth:`zipWithIndex`.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the zipped key-UniqueId pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.zip`\n |      :meth:`RDD.zipWithIndex`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  context\n |      The :class:`SparkContext` that this RDD was created on.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`SparkContext`\n |          The :class:`SparkContext` that this RDD was created on\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.context\n |      <SparkContext ...>\n |      >>> rdd.context is sc\n |      True\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __orig_bases__ = (typing.Generic[+T_co],)\n |  \n |  __parameters__ = (+T_co,)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from typing.Generic:\n |  \n |  __class_getitem__(params) from builtins.type\n |  \n |  __init_subclass__(*args, **kwargs) from builtins.type\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n\n"
     ]
    }
   ],
   "source": [
    "help(my_rdd)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3737989515766213,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tutorial_2_RDD_Basics (2)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
